\section{Related Work}

Quantifying exact latency for erasure-coded storage system is an open problem. Recently, there has been a number of attempts at finding latency bounds for an erasure-coded storage system \cite{MG1:12,Joshi:13,MDS-Queue,CS14,Xiang:2014:Sigmetrics:2014,Yu_TON,Yu-ICDCS,Yu-CCGRID}. In this paper, we utilize the probabilistic scheduling policy developed in \cite{Xiang:2014:Sigmetrics:2014,Yu_TON} and analyze the impact of caching on the service latency of erasure-coded storage.

Caches are a critical resource in data centers; however, there is no work on caching for erasure coded distributed storage. The problem of online cache management (i.e., decisions for evicting a data chunk currently in the cache to make room for a new chunk) has been studied for networks with distributed caches \cite{td_caches_1,td_caches_2}.  Cache replacement strategy called LRU (Least Recently Used) is widely used
in managing buffer cache  due to its simplicity \cite{1453496,Megiddo:2003:ASL:1090694.1090708,Zhou:2001:MRA:647055.715773,Pelc07,td_lru}. Recently, a steady-state characterization of various cache policies is developed in \cite{td_steady}, and new coded caching schemes to enable multicast opportunities for bandwidth minimization are proposed in \cite{td_cache,td_coded}. 

Much of the work on caching in data centers is focused on specialized application caches, such as Facebook's photo-serving
stack \cite{Huang:2013:AFP:2517349.2522722}, Facebook's social graph store \cite{180185}, memcached
\cite{Fitzpatrick:2004:DCM:1012889.1012894}, or explicit cloud caching services \cite{Chockler:2010:DCC:1859184.1859190,6097162}. However, in the presence of coding, new challenges arise. First, the chunks are distributed over multiple servers and a part of the chunks can be in the cache. Thus, it is not necessary for the complete file to be in the cache. Second, the latency calculation for a file depends on the placement of the files and the request of the files from $k$ out of $n$ servers. In this paper, we deal with these challenges to consider a novel caching strategy. To the best of our knowledge, this is the first work on caching with erasure coded files on distributed storage servers accounting for the latency in file retrievals, based on the estimated arrival rates.

Coded caching for a single server with multi-cast link to different users has been considered in \cite{DBLP:journals/corr/NiesenM14,6875212,6763007,6849235}. This does not account for multiple distributed storage servers, and latency to get the content. An extension of the approach to distributed storage systems is considered recently in \cite{DBLP:journals/corr/ShariatpanahiMK15}, where  multiple cache-enabled clients connected to multiple servers through an intermediate network. However, the impact of coding on the servers, and limited service rate of different servers is not taken into account. The key idea in this set of works uses a coded version of different files in the cache which helps in the case when users request different files with certain probabilities. The gain of the approach is due to the model where a message from the server can be received at multiple nodes and thus combined with coded content in the cache, one chunk from the server can help give a chunk for different files at different clients. In this paper, we do not have multicast links to different clients and thus coding across files in the cache is not used. 

Functional repair was introduced in \cite{Dimakis:10} for repairing a failed chunk with a new chunk such that the storage code satisfies the same properties even after repair. Thus, the replaced content can be different. In this paper, we use functional caching to have chunks in cache such that the file can be recovered from any of $k$ chunks from a combination of disk and cache contents where $(n,k)$ code is used for the files in the storage system. 