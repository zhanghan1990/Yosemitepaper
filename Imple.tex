\section{Numerical Results, Implementation and Evaluation}

In this section, we evaluate our proposed algorithm for functional cache optimization, through both simulation and implementation in an open-source, distributed filesystem.

\subsection{Simulation Setup}
We simulated our algorithm in a cluster of $m=12$ storage servers  holding $r=1000$ files of size 100 MB each using a (7,4) erasure code. Unless stated otherwise, cache size remains as 500 times of the chunk size (i.e., 500 times of 25 MB). The arrival rate for each file is set at a default value of $\lambda_i=$ 0.000156/sec,  0.000156/sec, 0.000125/sec, 0.000167/sec, 0.000104/sec for every five out of the 1000 files of each size. It gives an aggregate arrival rate of all files to be 0.1416/sec. The inverse of mean service times  for the 12 servers are set based on measurements of real service time in the distributed storage system Tahoe-lafs (which will be introduced later), and they are \{0.1, 0.1, 0.1, 0.0909, 0.0909, 0.0667, 0.0667, 0.0769, 0.0769, 0.0588, 0.0588\} for the 12 storage servers respectively. The placement of files on the servers is chosen at random, unless explicitly specified. The major objective of our simulation-based evaluation is to validate our latency analysis of erasure-coded storage with caching in those areas that are hard to implement in a real test-bed or are difficult to measure in experiments.
\subsection{Simulation Results}

{\bf Convergence of Algorithm.} We implemented our cache optimization algorithm using MOSEK, a commercial optimization solver, to project the gradient descent solution to the feasible space for  Prob\_$\Pi$. For 12 distributed storage servers in our testbed, Figure \ref{fig:conv} demonstrates the convergence of our algorithm in one time-bin, which optimizes the average latency of all files over request scheduling $\pi_{i,j,t}$ and cache placement $d_{i,t}$ for each file. We assume 1000 files, each using a (7,4) erasure code and of size 100 MB, divided evenly into five groups with the arrival rates of the five groups as mentioned above. The convergence of the proposed  algorithm is depicted in Fig. \ref{fig:conv} for cache size $C\times 25$ MB. For $C=4000$, four chunks of each file can be in the cache. A random initialization is chosen for $C=100$, while the converged solution for $C=100$ is taken as initialization for $C=200$ and so on. We note that the algorithm converges within a few iterations, and converges in less than 20 iterations with a threshold of 0.01 on latency for all cache size values in Figure \ref{fig:conv}.

 %Our algorithm efficiently solve the optimization problem with $r = 1000$ files of file size 100M, as the figure shows that average latency has been improved significantly when comparing the converged latency with the latency at iteration 1. The figure plots average latency of the 1000 files within one time bin with the designated request arrival rate for each file. It is observed that the normalized objective converges within 437 iterations for a tolerance $\epsilon = 0.001$. To achieve dynamic cache placement management for files stored in distributed storage systems, our optimization algorithm can be executed repeatedly upon beginning for different time bins.

\begin{figure}[!thbp]
\vspace{-2mm}
\begin{center}
{\includegraphics[trim=1.65in 3.3in 1.8in 3.5in, clip, width=.4\textwidth, draft=false]{./figs/conv.pdf}}
\vspace{-3mm}
\caption{Convergence of the proposed algorithm for a system with $r = 1000$ files each of size 100M and using a cache size of $C\times 25$M. The algorithm efficiently computes a solution in less than 20 iterations with a low tolerance $\epsilon = 0.01$ for different cache sizes.  A random initialization is chosen for C = 100, while the converged solution for C = 100 is taken as initialization for C = 200 and so on.}
\label{fig:conv}
\end{center}
\vspace{-.2in}
\end{figure}

{\bf Impact of Cache Size.} To validate that our proposed algorithm can effectively update file content placement in cache to optimize average latency in the storage system, we plot average latency for $r=1000$ files of size 100MB in one time bin with designated request arrival rate for each file, while cache size varies from 0 to 4000 times of the chunk-size. Fig \ref{fig:cache} shows that the average latency decreases as cache size increases, where average latency is 23 sec when no file has any content in cache, and is 0 sec when cache size is 4000 chunk-size since 4 chunks of each file can be in the cache. We note that the latency is convex decreasing function of the cache size, depicting that our algorithm is effectively updating content in cache and showing diminishing returns in decrease of latency after reaching certain cache size.


%And latency decreasing is faster than linear when cache size $C \leq 1500Gb$, as with a 100M file size and a (7,4) erasure code, the content of files are quickly filling up available cache and improve latency, however, when cache size continues to increase, the files that are frequently accessed already has enough content in cache, keep adding content which are not frequently accessed might will increase average latency, but not as much, that's why average latency decrease much more slowly when $C \geq 1500Gb$. In Fig \ref{fig:cache}, each data point in the figure represents the average latency of 1000 files within one time bin with a certain designated request arrival rate for each file, with a certain cache size (0Gb:50Gb:4000Gb), while all other parameters stays the same. The figure shows that our algorithm is effectively updating content in cache when more cache space is available, the large the cache size is, the more content of files the algorithm will put into cache, the less content it needs to retrieve from disk at the storage nodes, the smaller the average latency in the system becomes.
\begin{figure*}[!th]
\vspace{-3mm}
\begin{minipage}{0.31\textwidth}
\begin{center}
%{\includegraphics[trim =1.6in 2.5in 1.8in 3.55in, clip, width = \textwidth, draft=false]{./figs/cache.pdf}}

{\includegraphics[trim =1.6in 2.5in 1.8in 3.55in, width = \textwidth, draft=false]{./figs/cache.pdf}}

\vspace{-13mm}
\caption{Average latency as cache size varies from 0 to 4000 chunk-sixe, each data point represents average latency for $r=1000$ files in one time bin. This clearly demonstrates that the average latency decreases as cache size increases, when all other parameters are fixed. }
\label{fig:cache}
\end{center}
\end{minipage}
\hspace{0.2cm}
%\vspace{3mm}
\begin{minipage}{0.31\textwidth}
\begin{center}
{\includegraphics[trim =0.3in 1.1in 0.4in 2.4in, clip, width = \textwidth, draft=false]{./figs/cache_change.pdf}}
\vspace{-17mm}
\caption{Evolution of cache content in three time bins, each having different request arrival rates for the $r=10$ files. It shows that our algorithm is updating content held in cache according to the intensity of workload of each file effectively. }
\label{fig:cache_change}
\end{center}
\end{minipage}
\hspace{0.2cm}
%\vspace{3mm}
\begin{minipage}{0.34\textwidth}
\begin{center}
{\includegraphics[trim = 1.7in 2.2in 1.8in 3.5in, clip, width = \textwidth, draft=false]{./figs/arrrate_chunk.pdf}}
\vspace{-20mm}
\caption{The chunk placement depends not only on arrival rates, but also on placement of contents in the storage nodes and the service rates.  }
\label{fig:arrrate_chunk}
\end{center}
\end{minipage}
\vspace{-.2in}
\end{figure*}

{\bf Evolution of Cache Content Placement.} We validate that our algorithm minimizes service latency by optimizing the cache content with respect to request arrival rates (the algorithm is minimizing latency bound $\bar{U}_{i,t}$, and $\lambda_i$ and $\Lambda_j$ is playing an important role in $\bar{U}_{i,t}$), i.e., placing more file chunks that are frequently accessed (with a higher arrival rate) in cache. We design a simulation which consists of 10 files, of size 100MB, and run our algorithm for three time bins, where each time bin has a different request arrival rate for the $r=10$ files as given in Table \ref{tab:arr}. The arrows indicate the increase or decrease of file arrival rate in the consecutive time bins.
\begin{table*}
\caption{Table of arrival rates for 10 files in 3 time bins.} \label{tab:arr}
\centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
    \hline
    Time Bin & File 1 & File 2 & File 3 & File 4 & File 5 & File 6 & File 7 & File 8 & File 9 & File 10 \\ \hline
    1 & 0.000156	& 0.000156 & 0.000125 & 0.000167 & 0.000104 & 0.000156 & 0.000156 & 0.000125 & 0.000167 & 0.000104 \\ \hline
    2 & 0.000156	& 0.000156 & 0.000125 & 0.000125 \color{red}{$\downarrow$} & 0.000125 \color{blue}{$\uparrow$} & 0.000156 & 0.000156 & 0.000125 & 0.000125 \color{red}{$\downarrow$} & 0.000125 \color{blue}{$\uparrow$} \\ \hline
    3 & 0.000125 \color{red}{$\downarrow$} & 0.00025 \color{blue}{$\uparrow$} & 0.000125 & 0.000167 & 0.000104 & 0.000125 \color{red}{$\downarrow$} & 0.00025 \color{blue}{$\uparrow$} & 0.000125 & 0.000167 & 0.000104 \\
    \hline
    \end{tabular}
    \vspace{-.25in}
\end{table*}
In this simulation we plot the cache placement for the 10 files in steady state (after the algorithm converges) for each time bin. From Figure \ref{fig:cache_change}, we see that in the first time bin, as file 4 and file 9 are having the highest request arrival rates, files 4 and  9 have the highest number of chunks in cache in the steady state. In the second time bin, as  the arrival rate of files 4 and 9 decreased, and  that of files 5 and 10 increased, now the files that have the highest request arrival rate becomes files 1, 2, 6, and 7 (equalized arrival rate among these four). Thus Fig \ref{fig:cache_change} shows that in the second time bin these four files dominates the cache's steady state. In the third time bin, the files that have the highest arrival rate becomes files 2, 7, 4, and 9, and Fig. \ref{fig:cache_change} also shows that cache is mainly holding content of file 2, 7, and 9. Due to different service rates of the servers and the randomized placement of contents in the servers, it is not always the case that files with highest arrival rate need to be completely in the cache, and thus it becomes necessary to find how many chunks of each file should be placed in the cache.
%This figure has shown that our algorithm is effectively updating the content placement of all files in cache according to the goal of minimizing average latency in the storage system.

%\begin{figure}
%\begin{center}
%{\includegraphics[trim = 1.7in 2.2in 1.8in 3.5in, clip, width = .45\textwidth, draft=false]{./figs/arrrate_chunk.pdf}}
%\vspace{-20mm}
%\caption{The chunk placement not only depend on arrival rate, but also on placement of contents in the disk and the service rates.  }
%\label{fig:arrrate_chunk}
%\end{center}\vspace{-.4in}
%\end{figure}

{\bf Impact of Content Placement and Arrival Rate.} We note that in addition to the arrival rates of the files, placement of content on the storage nodes influence the decision of cache placement. We consider 10 files, of size 100M, using (7,4) erasure code are placed on 12 servers as described in the simulation set up. The first three files are placed on the first seven servers while the rest of the files are placed on the last seven servers. Note that servers 6 and 7 host chunks for all files. For the arrival rates of the files, we fix the arrival rate for the last eight files such that the arrival rate of the third and fourth file is $.0000962$/sec, and of the last six files is 0.0001042/sec. The arrival rates for the first two files are assumed to be the same, and due to symmetry of files and placement, we consider four categories of content placement in the cache - contents for the first two files, contents for 3rd file, contents for 4th file, and the contents for the last six files. In the considered arrival rates in Figure \ref{fig:arrrate_chunk}, there was no chunk for the third and the fourth file placed in the cache due to low arrival rates. We note that we always assume the arrival rates of the first two files as the highest but since the servers on which they are placed have relatively lower average load, the arrival rate needs to be significantly higher for them to increase content in the cache. The six bars in Figure \ref{fig:arrrate_chunk} correspond to arrival rates for the first two files of $0.0001250,    0.0001563,    0.0001786,    0.0002083,    0.0002500,$ and     0.0002778, respectively. At an arrival rate of $.000125$/sec for the first two files, there is no content for these files in the cache. Thus even though the arrival rate for the first two files is the largest, they do not need to be placed in the cache due to the specific placement of the contents in the storage servers that are lightly loaded.  As the arrival rate increases to $.00015625$/sec, two chunks corresponding to these files start to appear in the cache. Further increase of the arrival rates leads to more and more chunks in the cache. Thus, we note that the placement of chunks on the storage nodes, arrival rates, and service rates all play a role in allocating chunks of different files in the cache. Also, it may be sub-optimal to place a complete file in the cache, and thus our algorithm considers the option of choosing partial instead of complete files for the cache.

%\vspace{-.1in}
\subsection{Tahoe Testbed}
We have validated the effectiveness of our latency analysis and the algorithm for joint optimization on average latency in erasure-coded storage systems with caching through simulation. In the rest of this section we will evaluate the performance of our solution by implementing our algorithm in {\em Tahoe-Lafs ( v1.10.2)}, which is an open-source, distributed filesystem based on the {\em zfec} erasure coding library \cite{tahoe}. It provides three special instances of a generic {\em node}: (a)  {\em Tahoe Introducer}: it keeps track of a collection of storage servers and clients and introduces them to each other.   (b) {\em Tahoe Storage Server}: it exposes attached storage to external clients and stores erasure-coded shares.  (c) {\em Tahoe Client}: it processes upload/download requests and connects to storage servers through a Web-based REST API and the Tahoe-LAFS (Least-Authority File System) storage protocol over SSL.

Our algorithm requires dynamically updating cache placement in the storage system, but {\em Tahoe} does not support caching for now. In order to deal with this, we change the erasure code for the data in the storage nodes and thus the requirement of data from the storage nodes is reduced by the amount of file already present in the cache. For instance, if file $i$ of size $|F|$ is using $(n,k)$ erasure code and $d\le k$ chunks of the file are in the cache, we need $k-d$ our of $n$ chunks from storage nodes. Thus,  we assume that the (equivalent) code on storage nodes is $(n,k-d)$ and thus only $k-d$ chunks will be requested. However, for the chunk-size to be the same, we have the effective file-size to be $|F|(k-d)/k$. In summary, we fake the storage as storing a file of size $|F|(k-d)/k$ using $(n,k-d)$ code and thus the amount of data in the cache is accounted. At the client, $k-d$ chunks from the storage nodes and $d$ chunks existing in the cache can be combined for file retrieval.

%, so we have to simulate the caching policy by using updated erasure codes for each file on the client side. It works like this: the algorithm outputs different cache placement for each file, which yields to a different erasure code.
%For instance, if file $i$ is using $(n,k)$ erasure code when requests are submitted, and the cache is already holding 2 chunks of file $i$ (size $2F/k$, F is the original file size), then the client only needs to download another $k-2$ chunks (size $F-2F/k$) from the storage nodes, thus the download request becomes to retrieve a file of size $F-2F/k$ with erasure code (n,k-2) for a Tahoe client. Thus the algorithm requires that each file to use a customized, unique erasure code.  While Tahoe uses a default $(10,3)$ erasure code, it supports arbitrary erasure code specification statically through a configuration file. Also when the number of chunks of file $i$ in the cache changes with time bins in the algorithm, we also need to update the corresponding erasure code in the configuration file of file $i$. For example, if file $i$ has 2 chunks in cache in the first time bin and we need file $i$ to have 4 instead of 2 chunks in cache in the second time bin, we will create another 2 chunks of file $i$ based on the $k$ chunks stored in storage nodes and place them in cache, so the erasure code of file $i$ becomes $(n+2,k)$. On the other hand, if the algorithm need file $i$ to have 0 instead of 2 chunks in cache in the second time bin, we will delete the 2 chunks in cache directly, and the erasure code of file $i$ stays as $(n,k)$.

Tahoe does not support a dynamic updates of configuration files. Each time the erasure code in a configuration file has been updated, the client node needs to be restarted to make this change effective. Thus, the dynamic implementation of the storage system with caching is an involved process which is left as a further work.

%As our experiment is especially time-sensitive and this stop-start policy of Tahoe takes time so that we implemented this dynamic-erasure codes policy on the Tahoe client node to submit requests with updated erasure code configuration.

In Tahoe, each file with $(n,k)$ erasure code is broken into $k$ chunks, each chunk is encrypted, and is then broken into a set of segments, where each segment consists of a certain number of blocks (depending on the block size).  Each segment is then distributed to (ideally) $n$ distinct storage servers. The set of segments on each storage server constitute a chunk. Thus, the file equivalently consists of $k$ chunks which are encoded into $n$ chunks and each chunk consists of multiple segments\footnote{If there are not enough servers, Tahoe will store multiple chunks on one sever. Also, the term ``chunk'' we used in this paper is equivalent to the term ``share'' in Tahoe terminology.}. For chunk placement, the Tahoe client randomly selects a set of available storage servers with enough storage space to store $n$ chunks. For server selection during file retrievals, the client first sends DYHB (Do You Have Block?) requests to the storage servers to check if they hold a chunk of the requested file. Once the client has located all the chunks (from the k servers that responds the fastest), it then retrieves all the k segments in order,  in the format of cipher-text, and then  reconstruct the file after decryption and decoding. This means that it tends to download chunks from the ``fastest'' servers purely based on round-trip times (RTT). In our proposed algorithm, once we know each file's chunk placement in cache, we consider RTT plus expected queuing delay and transfer delay as a measure of latency.
\begin{figure}[!thbp]
\vspace{-2mm}
\begin{center}
\scalebox{0.31}{\includegraphics[draft=false]{./figs/testbed.pdf}}
\vspace{-6mm}
\caption{Our Tahoe testbed with 12 storage nodes and a client in an OpenStack cluster of 4 hosts.}
\label{fig:testbed}
\end{center}
\vspace{-.3in}
\end{figure}

To be coherent with simulation setup, we deployed 12 Tahoe storage servers as medium-sized virtual machine (VM) instances in an OpenStack-based cluster, each instance has 2 VCPUs, 4GB of memory and a 200GB volume attached. Thus the total storage capacity for this Tahoe cluster is 2.4TB, which is enough to handle large number of files and file requests. The cluster has 4 hosts where hosts 1-3 hold 4 Tahoe storage servers each, while host 4 holds an XLarge client, which has 8 VCPUs and 16GB of RAM. We need a large instance for client node as we submit all file requests from this node, so the Tahoe client is running as a proxy for many local clients. The deployment is shown in Figure~\ref{fig:testbed}.

\subsection{Implementation and Evaluation}
{\bf Service Time Distribution} While our service delay bound applies to arbitrary distribution and works for systems hosting any number of files, we first run an experiment to understand actual service time distribution on our testbed, which is used as an input of chunk service time in the algorithm. To find the chunk service time distribution for different chunk sizes,  we upload $r=1000$ files using (7,4) erasure code, and request the files as a Poisson process with an average arrival rate (among the 1000 files) of 0.0301/sec. In two separate experiments, we consider different chunk sizes of $25$ MB and $50$ MB, respectively.   We collect the results and plot the measurements of chunk service times.  Figure \ref{fig:cdf} depicts the Cumulative Distribution Function (CDF) of the chunk service times for requests with the two chunk sizes. Using the measured results, we get the mean service time of $12.4$ seconds for chunk of size $25$ M and $17.8$ seconds for chunk of size $50$ M.
% The reason the service time of 50MB is not twice that of 25MB is that it includes the transmission delay, which is constant.
We use the obtained mean, second and third moments from the service time distribution for finding cache placement in our algorithm. As an example, inverse  of the mean service time of \{0.0769, 0.0769, 0.0833, 0.0769, 0.0833, 0.0833, 0.0833, 0.0833, 0.0909, 0.1000, 0.0769, 0.0769\} is used for the 12 storage servers in the testbed, respectively. %Other moments for the service time were also found from the distribution to use in the optimization algorithm.
\begin{figure}[!thbp]
\vspace{-2mm}
\begin{center}
\scalebox{0.27}{\includegraphics[draft=false]{./figs/cdf.pdf}}
\vspace{-3mm}
\caption{Actual service latency distribution for different chunk sizes from the Tahoe testbed.}
\label{fig:cdf}
\end{center}
\vspace{-.1in}
\end{figure}

{\bf Evaluate the Performance of Our Solution.} As we have validated the algorithm in aspects of convergence and evolution of dynamic caching, we now evaluate whether the proposed algorithm improves average latency in storage systems. First, we choose $r=1000$ files using (7,4) erasure code, of size varying in \{100MB, 200MB, 500MB, 1000MB, 2000MB\}. We assume the request arrival forms a Poisson process, where the 1000 files are split into five classes of equal numbers of files, and the arrival rates within the five classes are \{0.0602/sec, 0.0602/sec, 0.01/sec, 0.0101/sec, 0.0101/sec\}, which yields an average arrival rate of 0.0301/sec. Cache size is fixed and set to be 2500 in the algorithm. In this case, each file can have 2500/1000=2.5 chunks on average in the cache. However, as the designed request arrival rates are higher for 40\% of the files (with arrival rate 0.602/sec), placing two or three chunks in cache would help reduce latency significantly, thus leading to uneven placement of chunks in the cache.  We compare average latency with and without caching. Fig \ref{fig:file_size} shows that average latency increases as the file size increases, which we can see from our latency bound. The figure also shows that our caching improves latency as compared to Tahoe's native storage system, which does not support caching, by 33\% on average. This improvement reduces as file size increases from 100MB to 500MB since the number of chunks that can be stored in the cache reduces (2500 chunks for 100MB files as compared to 500 chunks for 500MB files). Fig \ref{fig:file_size} shows that our algorithm with caching significantly improves latency with reasonable file and cache sizes.


\begin{figure*}[!th]
\vspace{-3mm}
\begin{minipage}{0.32\textwidth}
\begin{center}
{\includegraphics[width=\textwidth,draft=false]{./figs/file_size.pdf}}\vspace{-.2in}
\caption{Average latency for Tahoe storage system with and without caching for $r=1000$ files with an average arrival rate is 0.0301/sec. File sizes vary from 100MB to 2000MB, with a cache size fixed to 2500. Figure shows significant improvement in latency with caching. }
\label{fig:file_size}
\end{center}
\end{minipage}
\hspace{0.2cm}
%\vspace{3mm}
\begin{minipage}{0.32\textwidth}
\begin{center}
{\includegraphics[width=\textwidth,draft=false]{./figs/Arrival_rate.pdf}}\vspace{-.2in}
\caption{Comparison of average latency when our algorithm with caching  with Tahoe's native storage system without caching, with varying average arrival rates for $r=1000$ files of 200MB, where the cache size fixed at 2500.}
\label{fig:workload}
\end{center}
\end{minipage}
\hspace{0.2cm}
%\vspace{3mm}
\begin{minipage}{0.32\textwidth}
\begin{center}
{\includegraphics[trim= 1.4in 3.2in 1.6in 3.4in, clip, width = 0.8\textwidth, draft=false]{./figs/chunk_disccahe_time.pdf}}
\vspace{-.15in}
\caption{Number of chunk requests that are sent to storage servers or cache with an average arrival rate $\lambda_i=0.0225$ /sec and $\lambda_i=0.0384$, respectively, where a time bin of length 100 sec is divided into 20 equal time slots. The figure shows the dynamic nature of the number of chunks the client obtained from the cache/nodes. }
\label{fig:dynam}
\end{center}
\end{minipage}
\vspace{-.25in}
\end{figure*}
Next, we fix the file size to be 200MB and vary the file request arrival rates with average request arrival rate of the $r=1000$ files in \{0.0149/sec, 0.0225/sec, 0.0301/sec, 0.0384/sec, 0.0456/sec\}. The  cache size is fixed, and set to be 2500. Actual average service latency of files for each request arrival rate is shown by a bar plot in Figure~\ref{fig:workload}. In this experiment we also compare the caching based storage system using our algorithm for cache placement with Tahoe’s built-in native storage system without caching. Fig \ref{fig:workload} shows that our algorithm with caching outperforms Tahoe native storage in terms of average latency for all the average arrival rates. The proposed algorithm gives an average  49\% reduction in latency.  Thus, our algorithm with caching can mitigate traffic contention and reduce latency very efficiently compared to Tahoe’s native storage policy without caching.

{\bf Chunk Request Scheduling Evolution} In order to see how the request scheduling evolves during each time bin, we run the experiment with $r=1000$ files, each of size 200 MB and using a (7,4) erasure code, and a total cache size of 2500. The average file request arrival rate for the two experiments  is chosen as $\lambda_i=0.0225$ /sec and $\lambda_i=0.0384$, respectively. We divide each time bin (100 sec) into 20 time slots, each with length of 5 sec, and plot the number of chunks the client is getting from the cache and from the storage nodes in each time slot. Fig \ref{fig:dynam} shows that under both workloads, the number of chunks  retrieved from cache is smaller than that from storage nodes. As the cache size is $2500\times 25$ MB, with chunk size 50 MB, it has a capacity of 1250 chunks, which means each file has more chunks in storage nodes than that in cache on an average. Further, since the arrival rate of all file increases proportionally, the relative percentage of chunks retrieved from cache over 100s stays almost the same in the two cases, at about 33\%.
%\begin{figure}[!thbp]
%%\vspace{-2mm}
%\begin{center}
%{\includegraphics[trim= 1.4in 3.2in 1.6in 3.4in, clip, width = 0.4\textwidth, draft=false]{./figs/chunk_disccahe_time.pdf}}
%\vspace{-.15in}
%\caption{Number of chunk requests that are sent to storage servers or cache with an average arrival rate $\lambda_i=0.0225$ /sec and $\lambda_i=0.0384$, respectively, where a time bin of length 100 sec is divided into 20 equal time slots. The figure shows the dynamic nature of the number of chunks the client obtained from the cache/disk. }
%\label{fig:dynam}
%\end{center}
%\vspace{-.4in}
%\end{figure}

