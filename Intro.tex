\section{Introduction}

Erasure coding has seen itself quickly emerged as a promising technique to reduce the storage cost for a given reliability as compared to replicated systems \cite{2015_1,Dimakis:10}. It has been widely adopted in modern storage systems by companies like Facebook \cite{Sathiamoorthy13}, Microsoft \cite{Asure14} and Google \cite{Fikes10}. In these systems, the rapid growth of data traffic such as those generated by online video streaming, Big Data analytics, social networking and E-commerce activities has put a significant burden on the underlying networks of datacenter storage systems. Many researchers have begun to focus on latency analysis in erasure coded storage systems \cite{ISIT:12,Joshi:13,CS14,MDS-Queue,Xiang:2014:Sigmetrics:2014,Yu_TON} and to investigate algorithms for joint latency optimization and resource management \cite{Yu_TON,Yu-ICDCS,Yu-CCGRID}.

Historically, a key solution to relieve this traffic burden is caching \cite{td_cache}. By storing large chunks of popular data at different locations closer to end-users, caching can greatly reduce congestion in the network and improve service delay for processing file requests.   For example, Figure \ref{fig:proxy} shows a typical video storage architecture with video proxies and multiple video clients.  It is very common for 20\% of the video content to be accessed 80\% of the time, so caching popular content at proxies significantly reduces the overall latency on the client side.


\begin{figure}[!thbp]
\vspace{-2mm}
\begin{center}
\scalebox{0.2}{\fbox{\includegraphics[draft=false]{./figs/video-cache.pdf}}}
\vspace{-3mm}
\caption{Caching proxies for video storage to reduce the latency of client accesses for popular Video-on-Demand content }
\label{fig:proxy}
\end{center}
\vspace{-.25in}
\end{figure}


However, caching for data centers when the files are encoded with an erasure code has not been studied to the best of our knowledge. Further, the current results for caching systems cannot automatically be carried over to caches in erasure coded storage systems. First, using an $(n,k)$ maximum-distance-separable (MDS) erasure code, a file is encoded into $n$ chunks and can be recovered from any subset of $k$ distinct chunks. Thus, file access latency in such a system is determined by the delay to access file chunks on hot storage nodes with slowest performance. Significant latency reduction can be achieved by caching a few hot chunks (and therefore alleviating system performance bottlenecks), whereas caching additional chunks only has diminishing benefits. Second, caching the most popular data chunks is often optimal because the cache-miss rate and the resulting network load are proportional to each other. However, this may not be true for an erasure-coded storage, where cached chunks need not be identical to the transferred chunks. More precisely, a function of the data chunks can be computed and cached, so that the constructed new chunks, along with the existing chunks, also satisfy the property of being an MDS code. There have been caching schemes that cache the entire file  \cite{Nadgowda2014,Chang:2008:BDS:1365815.1365816,Zhu:2004:PSP:1006209.1006221}, while we can cache partial file  for an eraure-coded system (practically proposed for replicated storage systems in \cite{naik2015read}) which gives extra flexibility and the evaluation results depict the advantage of caching partial files.


%This functional construction approach has been recently introduced in \cite{td_func} for online repair of erasure-coded storage nodes.




%in the presence of coding, new challenges arise. First, the chunks are distributed over multiple servers and a part of the chunks can be in the cache. Thus, it is not necessary for the complete file to be in the cache. Second, the latency calculation for a file depends on the placement of the files and the request of the files from $k$ out of $n$ servers. In this paper, we deal with these challenges to consider a novel cache strategy. To the best of our knowledge, this is the first work on caching with erasure coded files on distributed storage servers accounting for the latency in file retrievals, based on the estimated arrival rates.

% http://www.eecs.berkeley.edu/~ramtin/icc2014.pdf
% https://people.cs.umass.edu/~elisha/Papers/INFOCOM12-final.pdf


In this paper, we propose a new {\em functional caching} approach called \textit{Sprout} that can efficiently capitalize on existing file coding in erasure-coded storage systems. In contrast to exact caching that stores $d$ chunks identical to original copies, our functional caching approach forms $d$ new data chunks, which together with the existing $n$ chunks satisfy the property of being an $(n+d,k)$ MDS code. Thus, the file can now be recovered from any $k$ out of $n+d$ chunks (rather than $k$ out of $n$ under exact caching), effectively extending coding redundancy, as well as system diversity for scheduling file access requests. The proposed functional caching approach saves latency due to more flexibility to obtain $k-d$ chunks from the storage system at a very minimal additional computational cost of creating the coded cached chunks. While quantifying service latency in erasure-coded storage systems is an open problem, we generalize previous results on probabilistic scheduling policy \cite{Yu_TON,Xiang:2014:Sigmetrics:2014} that distributes file requests to cache and storage nodes with optimized probabilities, and derive a closed-form upper bound on mean service latency for the proposed functional caching approach. The latency bound is obtained using order-statistic \cite{td_os} analysis and it works on erasure-coded storage systems with arbitrary cache size and data chunk placement. 

This analytical latency model for functional caching enables us to formulate a cache-content optimization problem. This problem is  an integer optimization problem, which is very difficult to solve. Towards this end, for given data chunk placement and file request arrival rates, we propose a heuristic algorithm that iteratively identifies files whose service latency benefits most from caching and constructs new functional data chunks until the cache is filled up. The algorithm can be efficiently computed to allow online cache optimization and management with time-varying arrival rates.

The proposed algorithm is an iterative algorithm, which converges within a few iterations in our conducted experiments and it was validated by the numerical results. For 1000 files, we find that the algorithm converges within 20 iterations. The file latency decreases as a convex function as the cache size increases thus showing diminishing returns for the increase in cache size. We also find that it is suboptimal in general to have all $k$ chunks of an $(n,k)$ coded file in the cache. Further, the cache placement depends heavily on the file arrival rates, storage server service time distributions, as well as the content placement on the files. If a high arrival-rate file is placed on servers which have less overall load, this file may not have any contribution in the cache. Thus, the proposed algorithm accounts for all the system parameters to effectively find the cache placement. The proposed algorithm is  prototyped using {\em Tahoe}, an open-source erasure-coded storage system \cite{tahoe} and tested on a real-world storage testbed. We find that caching helps improve the latency significantly.

%Finally, through prototype implementation using open source storage systems, we show that the proposed functional caching algorithm significantly outperforms the baseline exact caching algorithm.


The key contributions of our paper include:
\begin{itemize}
\item We propose a new functional caching scheme that leverages existing file coding in erasure-coded storage systems, and quantify its service latency through probabilistic request scheduling.
\item Based on the service latency analysis, an optimization problem is formulated which minimizes the average latency of the files. This problem has integer constraints due to the integer number of chunks in cache.
\item An iterative algorithm is developed to optimize cache content. The proposed algorithm takes file placement, service time distributions, and file arrival rates into account to find the cache placement which optimizes the service latency.
\item The simulation results show that the proposed algorithm converges within a few iterations.
\item The prototype implementation of the functional caching scheme and the cache optimization algorithm using {\em Tahoe} are used to validate significant latency reduction on a real-world storage testbed.
\end{itemize}
The remainder of this paper is organized as follows. Section II provides related work for this paper. In Section III, we describe the system model used in the paper with a description of  functional caching. Section IV formulates the cache optimization problem and develops an iterative algorithmic solution. Prototype and evaluation are included in Section V. Section VI concludes the paper.

