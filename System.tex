

\section{System Model}

We consider a distributed storage system consisting of $m$ heterogeneous storage nodes, denoted by $\mathcal{M}=\{1,2,\ldots,m\}$. To distributively store a set of $r$ files, indexed by $i=1,\ldots,r$, we partition each file $i$ into $k_i$ fixed-size chunks\footnote{While we make the assumption of fixed chunk size here to simplify the problem formulation, all results in this paper can be easily extended to variable chunk sizes. Nevertheless, fixed chunk sizes are indeed used by many existing storage systems \cite{DPR04,AJX05,LC02}.} and then encode it using an $(n_i,k_i)$ MDS erasure code to generate $n_i$ distinct chunks of the same size for file $i$. The encoded chunks are stored on the disks of $n_i$ distinct storage nodes. A set $\mathcal{S}_i$ of storage nodes, satisfying $\mathcal{S}_i\subseteq\mathcal{M}$ and $n_i=|\mathcal{S}_i|$ is used to store file $i$. Therefore, each chunk is placed on a different node to provide high reliability in the event of node or network failures. The use of $(n_i,k_i)$ MDS erasure code allows the file to be reconstructed from any subset of $k_i$-out-of-$n_i$ chunks, whereas it also introduces a redundancy factor of $n_i/k_i$.

The files are accessed by compute servers located in the same datacenter. A networked cache of size $C$ is available at each compute server to store a limited number of chunks of the $r$ files in its cache memory. File access requests are modeled by a non-homogenous Poisson process. We make the assumption of time-scale separation, such that system service time is divided into multiple bins, each with different request arrival rates, while the arrival rates within each bin remain stationary. Let $\lambda_{i,j,t}$ be the arrival rate of file-$i$ requests at compute server $j$ in time bin $t$. Since each cache serves a single compute server, we consider a separate optimization for each cache and suppress server index $j$ in the notations. Let $d_i\le k_i$ (chunks) be the size of cache memory allocated to storing file $i$ chunks. These chunks in cache memory can be both prefetched in an offline fashion during a placement phase \cite{td_cache} (during hours of low workload) and updated on the fly when a file $i$ request is processed by the system.

Under functional caching, $d_i$ new coded data chunks of file $i$ are constructed and cached, so that along with the existing $n_i$ chunks satisfy the property of being an $(n_i+d_i,k_i)$ MDS code. Therefore, for given erasure coding and chunk placement on storage nodes and cache, a request to access file $i$ can be processed using $d_i$ cached chunks in conjunction with $k_i-d_i$ chunks on distinct storage nodes. After each file request arrives at the storage system, we model this by treating the file request as a {\em batch} of $k_i-d_i$ chunk requests that are forwarded to appropriate storage nodes, as well as $d_i$ chunk requests that are processed by the cache. Each storage node buffers requests in a common queue of infinite capacity and process them in a FIFO manner. The file request is served when all $k_i$ chunk requests are processed. Further, we consider chunk service time ${\bf X}_j$ of node $j$ with {\em arbitrary distributions}, whose statistics can be inferred from existing work on network delay \cite{AY11,WK} and file-size distribution \cite{D11,PT12}.


%Each storage node $j$ has a cache of size $C_j$, which aids in the requests for clients who access data from server $j$. We assume that the content from cache in storage node $j$ cannot be transferred to other storage nodes, and only the content from the disc can be transferred. Each cache can have multiple chunks of a file.

%For known erasure coding and chunk placement in discs and caches, we shall now describe the client model of the distributed storage system. We assume that time can be divided into multiple bins, in which the client requests remain stationary. We note that these bins may not necessarily be the same time, and can depend on a trigger from the system that the arrival distribution has changed. We assume that the arrival of client requests for each file $i$ from a server $j$ form a non-homogenous Poisson process with a known rate $\lambda_{i,j,t}$ for time-period $t$. Since we will do the cache optimization for cache in server $j$, we ignore $j$ and just call $\lambda_{i,j,t}$ as $\lambda_{i,t}$, where it does not cause confusion. We consider chunk service time ${\bf X}_j$ of node $j$ with {\em arbitrary distributions}, whose statistics can be obtained inferred from existing work on network delay \cite{AY11,WK} and file-size distribution \cite{D11,PT12}. Under MDS codes, each file $i$ can be retrieved from any $k_i$ distinct nodes that store the file chunks. We model this by treating each file request as a {\em batch} of $k_i-d_i$ chunk requests from different discs, where $d_i$ chunks of file $i$ exist in the local cache, so that a file request is served when all $k_i-d_i$ chunk requests in the batch are processed by distinct storage nodes. All requests are buffered in a common queue of infinite capacity.



\begin{figure}[!thbp]
\vspace{-2mm}
\begin{center}
\scalebox{0.35}{\fbox{\includegraphics[draft=false]{./figs/sys1.pdf}}}
\vspace{-3mm}
\caption{An illustration of functional caching and exact caching in an erasure-coded storage system with one file using a $(5,4)$ erasure code.}
\label{fig:conv}
\end{center}
%\vspace{-.3in}
\end{figure}


{\noindent \bf Example.} Consider a datacenter storing a single file using a $(5,4)$ MDS code. The file is split into $k_i=4$ chunks, denoted by $A_1,A_2,A_3,A_4$, and then linearly encoded to generate $n_i=5$ coded chunks $F_1=A_1$, $F_2=A_2$, $F_3=A_3$, $F_4=A_4$, and $F_5=A_1+A_2+A_3+A_4$ in a  finite field of order at-least 5. Two compute servers in the datacenter access this file and each is equipped with a cache of size $C=2$ chunks as depicted in Figure \ref{fig:conv}. The compute server on the right employs an exact caching scheme and stores chunks $F_1,F_2$ in the cache memory. Thus, 2 out of 3 remaining chunks (i.e., $F_3$, $F_4$ or $F_5$) must be retrieved to access the file, whereas chunks $F_1,F_2$ and their host nodes will not be selected for scheduling requests.


Under functional caching, the compute server on the left generates
$d_i=2$ new coded chunks, i.e., $C_1=A_1+2A_2+3A_3+4A_4$ and $C_2=4A_1+3A_2+2A_3+1A_4$, and saves them in its cache memory. It is easy to see that chunks $F_1,\ldots,F_5$ and $C_1,C_2$ now form a $(7,4)$ erasure code. Thus, the file can be retrieved by accessing $C_1,C_2$ in the cache together with any 2 out of 5 chunks from $F_1,\ldots,F_5$. This allows an optimal request scheduling mechanism to select the least busy chunks/nodes among all 5 possible candidates in the system, so that the service latency is determined by the best 2 storage node with minimum queuing delay. In contrast, service latency in exact caching is limited by the latency of accessing a smaller subset of chunks $F_3,F_4,$ and $F_5$.

In order to have a $(n,k)$ coded file in the storage server, we can construct chunks by using an $(n+k,k)$ MDS code, where $n$ chunks are stored in the storage server. The remaining $k$ out of the $n+k$ coded chunks are assigned to be in part in cache based on the contents of the file in the cache. Thus, irrespective of the value of $d\le k$, we ascertain that $(n+d,k)$ code, formed with $n$ coded chunks in the storage server and $k$ coded chunks in the cache, will be MDS. 